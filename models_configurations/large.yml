model:
  tokenizer_path: tokenizer_250k
  layer_norm_eps: 1.0e-05
  output_past: True
  type_vocab_size: 1
  max_length: 256
  hidden_size: 768
  num_attention_heads: 6
  num_hidden_layers: 10
  intermediate_size: 3072

training:
  gradient_accumulation_steps: 2
  resume_training: True
  ignore_data_skip: False
  train_from_scratch: False
  use_whole_word_mask: False
  lang_sampling_factor: 1.0
  overwrite_output_dir: False
  seed: 1234
  max_steps: 50000
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  dataloader_num_workers: 6
  fp16: True
  save_steps: 2000
  save_total_limit: 1
  learning_rate: 0.0001
  warmup_steps: 4000

data:
  train: data/train/
  eval:
    all: data/eval/all_eval.txt 
    per_lang: data/eval/